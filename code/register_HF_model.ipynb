{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Jupyter notebook log a transformer model on Hugging Face\n",
    "\n",
    "This notebook is a simplistic implementation of the tutorial [here](https://mlflow.org/docs/latest/llms/transformers/tutorials/text-generation/text-generation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import transformers\n",
    "import mlflow\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers pipeline\n",
    "In the following step, give a name to your task and define a `transformer pipeline`. In the parameter `model`, write the name of the hugging face model you want to use. For this demo, we define a text2text-generation task using the model [declare-lab/flan-alpaca-large](https://huggingface.co/declare-lab/flan-alpaca-large)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed302c3558e49ba86350ae9eb1a816e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/787 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0235187e17ef4d0db37ad1647a634f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d57fa2f24d49ff93e59a6806eb81de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea2964c257448c084d78d8dd11e694f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed1518e8efd435094f455ffbd06db55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739ec902fa934f5faf3586868e71fd0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the task that we want to use (required for proper pipeline construction)\n",
    "task = \"text2text-generation\"\n",
    "\n",
    "# Define the pipeline, using the task and a model instance that is applicable for our task.\n",
    "generation_pipeline = transformers.pipeline(\n",
    "    task=task,\n",
    "    model=\"declare-lab/flan-alpaca-large\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Unknown task time series, available tasks are ['audio-classification', 'automatic-speech-recognition', 'conversational', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-to-image', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime series\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define the pipeline, using the task and a model instance that is applicable for our task.\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m generation_pipeline2 \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamazon/chronos-t5-small\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/__init__.py:860\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m         pipeline_class \u001b[38;5;241m=\u001b[39m get_class_from_dynamic_module(\n\u001b[1;32m    854\u001b[0m             class_ref,\n\u001b[1;32m    855\u001b[0m             model,\n\u001b[1;32m    856\u001b[0m             code_revision\u001b[38;5;241m=\u001b[39mcode_revision,\n\u001b[1;32m    857\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[1;32m    858\u001b[0m         )\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 860\u001b[0m     normalized_task, targeted_task, task_options \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pipeline_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    862\u001b[0m         pipeline_class \u001b[38;5;241m=\u001b[39m targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimpl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/__init__.py:543\u001b[0m, in \u001b[0;36mcheck_task\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_task\u001b[39m(task: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, Dict, Any]:\n\u001b[1;32m    499\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;124;03m    Checks an incoming task string, to validate it's correct and return the default Pipeline and Model classes, and\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;124;03m    default models if they exist.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m \n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPIPELINE_REGISTRY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1324\u001b[0m, in \u001b[0;36mPipelineRegistry.check_task\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m task, targeted_task, (tokens[\u001b[38;5;241m1\u001b[39m], tokens[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid translation task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation_XX_to_YY\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1324\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, available tasks are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_supported_tasks()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation_XX_to_YY\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1326\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unknown task time series, available tasks are ['audio-classification', 'automatic-speech-recognition', 'conversational', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-to-image', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']\""
     ]
    }
   ],
   "source": [
    "# Define the task that we want to use (required for proper pipeline construction)\n",
    "task = \"time series\"\n",
    "\n",
    "# Define the pipeline, using the task and a model instance that is applicable for our task.\n",
    "generation_pipeline2 = transformers.pipeline(\n",
    "    task=task,\n",
    "    model=\"amazon/chronos-t5-small\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLflow set up\n",
    "In the following cell, we name the experiment, run, `artifact_path`, and name with which we want to register the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment with name HuggingFace exists. Loading it...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/tmp/mlflow/db/203237740360877547', creation_time=1715163911885, experiment_id='203237740360877547', last_update_time=1715163911885, lifecycle_stage='active', name='HuggingFace', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name = 'HuggingFace'\n",
    "run_name = 'test_alpacav3'\n",
    "artifact_path = 'text_generator'\n",
    "registered_model_name = 'text_generator'\n",
    "# Remote location of the S3 bucket (on AWS)\n",
    "# You should have defined this as a custom key \n",
    "# in your environment\n",
    "s3_bucket=os.environ['CUSTOM_KEY']\n",
    "# Location to store the ML experiments locally\n",
    "# This is also the location that you sync with the\n",
    "# S3 bucket (see below)\n",
    "tracking_uri = '/tmp/mlflow/db/'\n",
    "# Sync all contents from the S3 bucket (remote) to the local location\n",
    "os.system(f\"aws s3 sync {s3_bucket} {tracking_uri} --quiet\")\n",
    "# Let mlflow where you are storing your ML experiments\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "# If the expr_name is not already in use, create one\n",
    "does_experiment_exist = mlflow.get_experiment_by_name(experiment_name)\n",
    "if not does_experiment_exist:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "else:\n",
    "    print (f'Experiment with name {experiment_name} exists. Loading it...')\n",
    "# If the expr_name is already in use, use it to track\n",
    "# your MLflow\n",
    "mlflow.set_experiment(experiment_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log and register the model\n",
    "Log and register the model by using the package `mlflow.transformers` provided by MLflow. We will use the function `log_model` to log and register your model. Please note that we set the parameter `save_pretrained` to `False` because we want MLflow to just remember the reference of the model to the HuggingFace Hub. This especially useful when the pretrained model is too big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_124/1003482068.py:2: FutureWarning: The 'transformers' MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.39.3``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.\n",
      "  model_info = mlflow.transformers.log_model(\n",
      "/opt/conda/lib/python3.10/site-packages/mlflow/models/model.py:625: FutureWarning: The 'transformers' MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.39.3``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.\n",
      "  flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\n",
      "2024/05/08 14:48:08 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained is set to False. The reference to HuggingFace Hub repository declare-lab/flan-alpaca-large will be logged instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c7b955689f4fdfb1a02055e8d1bc78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449547886a8c410e8848469de18e9474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'text_generator' already exists. Creating a new version of this model...\n",
      "Created version '6' of model 'text_generator'.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    model_info = mlflow.transformers.log_model(\n",
    "        transformers_model=generation_pipeline,\n",
    "        artifact_path=artifact_path,\n",
    "        registered_model_name=registered_model_name,\n",
    "        # input_example=input_example,\n",
    "        # signature=signature,\n",
    "        # Uncomment the following line to save the model in 'reference-only' mode:\n",
    "        save_pretrained=False,\n",
    "    )\n",
    "    # extract the run_id\n",
    "    # (run_name and run_id are differnt things. \n",
    "    # while run_id is unique to a run, differnt runs can have same run_names)\n",
    "    run_id = run.info.run_id\n",
    "print (run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log metrics\n",
    "Log some dummy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy metrics\n",
    "metrics = {\"mse\": 2500.00, \"rmse\": 50.00}\n",
    "\n",
    "# Log a batch of metrics to the run_id above\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    mlflow.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLflow sync (IMPORTANT)\n",
    "Once your ML experiment has ended, please sync your local copy with the S3 bucket.\n",
    "Failure to do so will lead to loss of experiment logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sync the local contents with the S3 bucket\n",
    "os.system(f\"aws s3 sync {tracking_uri} {s3_bucket} --quiet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "In this step we load the model back from MLflow for inference purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024/05/08 14:23:05 WARNING mlflow.transformers.model_io: Could not specify device parameter for this pipeline type.Falling back to loading the model with the default device.\n"
     ]
    }
   ],
   "source": [
    "# Load our pipeline as a generic python function\n",
    "sentence_generator = mlflow.pyfunc.load_model(model_info.model_uri)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, we define the input `data` (which should be a text as a vector/array), make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The capital of Germany is Berlin.', 'The company running German Railways is Deutsche Bahn.']\n"
     ]
    }
   ],
   "source": [
    "# Validate that our loaded pipeline, as a generic pyfunc, can produce an output that makes sense\n",
    "predictions = sentence_generator.predict(\n",
    "    data=[\n",
    "        \"What is the capital of Germany?\",\n",
    "        \"Please tell me the name of the company running German Railways.\",\n",
    "    ]\n",
    ")\n",
    "print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
